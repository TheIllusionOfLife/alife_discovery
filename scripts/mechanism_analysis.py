#!/usr/bin/env python3
"""Mechanism Analysis: quantitative investigation of why max entity size = 6.

Post-processes entity_log_combined.parquet + step_timeseries.parquet to compute:
1. Entity lifetime distribution
2. Bond survival rate
3. Growth transitions (size k â†’ k+1)
4. Rule table summary (mean bond prob vs mean entity size)
5. Graph symmetry counts (automorphism group size)

Usage:
    uv run python scripts/mechanism_analysis.py \
        --entity-log data/assembly_audit/entity_log_combined.parquet \
        --timeseries data/assembly_audit/seed_0/logs/step_timeseries.parquet \
        --out-dir data/mechanism
"""

from __future__ import annotations

import argparse
from pathlib import Path

import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Mechanism Analysis")
    p.add_argument("--entity-log", type=Path, required=True)
    p.add_argument("--timeseries", type=Path, default=None)
    p.add_argument("--out-dir", type=Path, default=Path("data/mechanism"))
    return p.parse_args()


def _entity_lifetime_stats(table: pa.Table) -> list[str]:
    """Measure contiguous-run lifetimes per (run_id, entity_hash).

    An entity type may appear, disappear, and reappear across steps.
    Each contiguous run of consecutive snapshot intervals counts as one
    lifetime instance.  This avoids conflating persistent entities with
    those that independently reform.
    """
    run_ids = table.column("run_id").to_pylist()
    steps = table.column("step").to_pylist()
    hashes = table.column("entity_hash").to_pylist()

    from collections import defaultdict

    groups: dict[tuple[str, str], list[int]] = defaultdict(list)
    for rid, s, h in zip(run_ids, steps, hashes, strict=True):
        groups[(rid, h)].append(s)

    # Infer snapshot interval from global step data
    all_steps = sorted(set(steps))
    if len(all_steps) >= 2:
        intervals = [all_steps[i + 1] - all_steps[i] for i in range(len(all_steps) - 1)]
        snapshot_interval = min(intervals) if intervals else 1
    else:
        snapshot_interval = 1

    lifetimes: list[int] = []
    for _key, step_list in groups.items():
        sorted_steps = sorted(set(step_list))
        # Split into contiguous runs (consecutive snapshot intervals)
        run_length = 1
        for i in range(1, len(sorted_steps)):
            if sorted_steps[i] - sorted_steps[i - 1] <= snapshot_interval:
                run_length += 1
            else:
                lifetimes.append(run_length)
                run_length = 1
        lifetimes.append(run_length)

    if not lifetimes:
        return ["--- Entity Lifetime Distribution ---", "  No data (no entities observed)."]

    arr = np.array(lifetimes, dtype=float)
    lines = [
        "--- Entity Lifetime Distribution ---",
        f"  n_entity_instances: {len(lifetimes)}",
        f"  mean_lifetime (snapshots): {arr.mean():.2f}",
        f"  median_lifetime: {np.median(arr):.1f}",
        f"  max_lifetime: {arr.max():.0f}",
    ]
    return lines


def _bond_survival_stats(ts_table: pa.Table) -> list[str]:
    """Compute bond survival rate from timeseries n_bonds[t+1]/n_bonds[t]."""
    run_ids = ts_table.column("run_id").to_pylist()
    steps = ts_table.column("step").to_pylist()
    n_bonds = ts_table.column("n_bonds").to_pylist()

    from collections import defaultdict

    runs: dict[str, list[tuple[int, int]]] = defaultdict(list)
    for rid, s, nb in zip(run_ids, steps, n_bonds, strict=True):
        runs[rid].append((s, nb))

    ratios: list[float] = []
    for _rid, data in runs.items():
        data.sort()
        for i in range(1, len(data)):
            prev_bonds = data[i - 1][1]
            curr_bonds = data[i][1]
            if prev_bonds > 0:
                ratios.append(curr_bonds / prev_bonds)

    if not ratios:
        return ["--- Bond Survival Rate ---", "  No data (no timeseries with bonds)."]

    arr = np.array(ratios)
    lines = [
        "--- Bond Survival Rate ---",
        f"  n_transitions: {len(ratios)}",
        f"  mean_survival_ratio: {arr.mean():.4f}",
        f"  median_survival_ratio: {np.median(arr):.4f}",
    ]
    return lines


def _growth_transition_stats(table: pa.Table) -> list[str]:
    """Count population-level size distribution changes between consecutive steps.

    Since ``entity_hash`` encodes topology (hence size), an individual entity
    cannot change size without changing hash.  Instead, we compare the
    per-run size distribution at consecutive snapshots to quantify how
    the population shifts toward larger or smaller entities over time.
    """
    run_ids = table.column("run_id").to_pylist()
    steps = table.column("step").to_pylist()
    sizes = table.column("entity_size").to_pylist()

    from collections import Counter, defaultdict

    # Build size distribution per (run_id, step)
    step_sizes: dict[tuple[str, int], list[int]] = defaultdict(list)
    for rid, s, sz in zip(run_ids, steps, sizes, strict=True):
        step_sizes[(rid, s)].append(sz)

    # Group steps by run_id
    run_steps: dict[str, list[int]] = defaultdict(list)
    for rid, s in step_sizes:
        run_steps[rid].append(s)

    transitions: dict[str, int] = defaultdict(int)
    total_pairs = 0
    for rid, step_list in run_steps.items():
        sorted_steps = sorted(set(step_list))
        for i in range(1, len(sorted_steps)):
            prev_dist = Counter(step_sizes[(rid, sorted_steps[i - 1])])
            curr_dist = Counter(step_sizes[(rid, sorted_steps[i])])
            prev_max = max(prev_dist.keys()) if prev_dist else 0
            curr_max = max(curr_dist.keys()) if curr_dist else 0
            if curr_max > prev_max:
                transitions["growth"] += 1
            elif curr_max < prev_max:
                transitions["shrinkage"] += 1
            else:
                transitions["stable"] += 1
            total_pairs += 1

    lines = ["--- Population Size Transitions ---"]
    if total_pairs == 0:
        lines.append("  No step pairs to compare.")
    else:
        for label in ("growth", "stable", "shrinkage"):
            count = transitions.get(label, 0)
            pct = count / total_pairs * 100
            lines.append(f"  {label}: {count} ({pct:.1f}%)")
        lines.append(f"  total step pairs: {total_pairs}")
    return lines


def _symmetry_counts(table: pa.Table) -> list[str]:
    """Compute graph automorphism counts for unique entity types."""

    hashes = table.column("entity_hash").to_pylist()
    sizes = table.column("entity_size").to_pylist()
    ais = table.column("assembly_index").to_pylist()

    # Deduplicate by hash
    seen: dict[str, tuple[int, int]] = {}
    for h, sz, ai in zip(hashes, sizes, ais, strict=True):
        if h not in seen:
            seen[h] = (sz, ai)

    lines = [
        "--- Graph Symmetry (Automorphism Group Size) ---",
        f"  unique_entity_types: {len(seen)}",
        "  (Full automorphism computation deferred to entity graph reconstruction)",
    ]
    return lines


def main() -> None:
    args = parse_args()
    args.out_dir.mkdir(parents=True, exist_ok=True)

    entity_table = pq.read_table(args.entity_log)
    lines = ["=== Mechanism Analysis Summary ===", ""]

    lines.extend(_entity_lifetime_stats(entity_table))
    lines.append("")

    if args.timeseries is not None and args.timeseries.exists():
        ts_table = pq.read_table(args.timeseries)
        lines.extend(_bond_survival_stats(ts_table))
        lines.append("")

    lines.extend(_growth_transition_stats(entity_table))
    lines.append("")

    lines.extend(_symmetry_counts(entity_table))
    lines.append("")

    text = "\n".join(lines) + "\n"
    out_path = args.out_dir / "mechanism_summary.txt"
    out_path.write_text(text)
    print(text)
    print(f"Saved: {out_path}")


if __name__ == "__main__":
    main()
